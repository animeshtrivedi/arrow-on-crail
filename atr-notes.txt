https://arrow.apache.org/docs/java/


   java.lang.Thread.State: RUNNABLE
	at com.github.animeshtrivedi.benchmark.MemoryIOChannel.read(MemoryIOChannel.java:52)
	at org.apache.arrow.vector.ipc.ReadChannel.readFully(ReadChannel.java:57)
	at org.apache.arrow.vector.ipc.ReadChannel.readFully(ReadChannel.java:79)
	at org.apache.arrow.vector.ipc.message.MessageSerializer.deserializeRecordBatch(MessageSerializer.java:291)
	at org.apache.arrow.vector.ipc.ArrowFileReader.readRecordBatch(ArrowFileReader.java:162)
	at org.apache.arrow.vector.ipc.ArrowFileReader.loadNextBatch(ArrowFileReader.java:113)
	at org.apache.arrow.vector.ipc.ArrowFileReader.loadRecordBatch(ArrowFileReader.java:139)
	at com.github.animeshtrivedi.benchmark.ArrowReaderDebug.mode2(ArrowReaderDebug.java:143)
	at com.github.animeshtrivedi.benchmark.ArrowReaderDebug.run(ArrowReaderDebug.java:118)
	at com.github.animeshtrivedi.benchmark.ArrowMemoryBench.run(ArrowMemoryBench.java:47)
	at java.lang.Thread.run(Thread.java:748)

September 23,

- Java thread migration issue. There is a lot of thread migration in the benchmark. I don't
know why this is happening but for now this can be controlled with numactl -C [i-j] command.

- performance for 4k, binary blob (100k stepping, 2 million items = 8GB/thread)
  6.8, 12.7, 22.7, 19.6, 12.4 Gbps

profile:
 Performance counter stats for 'java -Xmn128G -Xmx256G -cp ./log4j/:./benchmark-arrow-1.0.jar:./dependency/* com.github.animeshtrivedi.benchmark.Main -t ArrowMemBench -p 16 -r 2000000 -c 1 -g 100000 -n binary -s 4096' (3 runs):

    3058677.145426      task-clock (msec)         #   11.392 CPUs utilized            ( +-  4.99% )
           673,436      context-switches          #    0.220 K/sec                    ( +- 13.84% )
             4,579      cpu-migrations            #    0.001 K/sec                    ( +-  2.11% )
       170,962,141      page-faults               #    0.056 M/sec                    ( +-  1.82% )
10,110,216,245,206      cycles                    #    3.305 GHz                      ( +-  4.98% )
 8,986,436,851,767      stalled-cycles-frontend   #   88.88% frontend cycles idle     ( +-  5.01% ) <======
 7,877,343,729,847      stalled-cycles-backend    #   77.91% backend cycles idle      ( +-  5.03% ) <======
 2,094,981,792,386      instructions              #    0.21  insn per cycle
                                                  #    4.29  stalled cycles per insn  ( +-  4.88% )
   433,303,383,610      branches                  #  141.664 M/sec                    ( +-  5.60% )
     1,312,678,881      branch-misses             #    0.30% of all branches          ( +-  3.46% )

     268.489563671 seconds time elapsed                                          ( +-  4.58% )


But this profile is for the full run, not just the benchmarking.

- same for 1kB buffer size, what is the performance for this?
   6.6, 13.3., 22.9, 37.22, 13.21 Gbps

- observation: quite a bit of time is spent in kernel (66%) vs 33% in userspace? why is this strange split. Even
for a single threaded design.

- performance collapses after 4 cores, lot more time in kernel.


12:57:12	 102% 		 66% 		 33%
12:57:13	 101% 		 66% 		 33%


    11.68%  java             perf-13514.map       [.] jlong_disjoint_arraycopy
     6.83%  java             [kernel.kallsyms]    [k] __handle_mm_fault
     5.74%  java             perf-13514.map       [.] Lcom/github/animeshtrivedi/benchmark/ArrowReader;::consumeBinary
     4.24%  java             [kernel.kallsyms]    [k] get_page_from_freelist
     2.99%  java             [kernel.kallsyms]    [k] __pagevec_lru_add_fn
     2.16%  java             [kernel.kallsyms]    [k] swapgs_restore_regs_and_return_to_usermode
     2.00%  java             [kernel.kallsyms]    [k] __alloc_pages_nodemask
     1.93%  java             [kernel.kallsyms]    [k] _raw_spin_lock
     1.79%  java             [kernel.kallsyms]    [k] __do_page_fault


* Does have a holder object here helps?

With the holder run
13:30:21	 101% 		 70% 		 29%
13:30:22	 100% 		 71% 		 28%
13:30:23	 101% 		 71% 		 28%

But the performance improves from ~7 Gbps -> 12.37 Gbps when spending more time in the kernel

WHY?
profile:
     8.52%  java             [kernel.kallsyms]   [k] __handle_mm_fault
     8.40%  java             perf-25763.map      [.] jlong_disjoint_arraycopy
     4.28%  java             perf-25763.map      [.] Lcom/github/animeshtrivedi/benchmark/ArrowReader;::consumeBinaryHolder
     3.82%  java             [kernel.kallsyms]   [k] get_page_from_freelist
     2.68%  java             [kernel.kallsyms]   [k] free_pcppages_bulk
     2.20%  java             [kernel.kallsyms]   [k] __mod_zone_page_state
     2.07%  java             [kernel.kallsyms]   [k] swapgs_restore_regs_and_return_to_usermode
     1.99%  java             [kernel.kallsyms]   [k] release_pages
     1.91%  perf             [kernel.kallsyms]   [k] unmap_page_range


* Does having a second run help? a bit so the bandwidth is up from 12.3 -> 13.7 Gbps, -3 seconds were saved. Not
conclusive, the next run just gave -1 seconds. But lets run in general with the warmup.

[cmd: numactl -C 0 ./run.sh -t ArrowMemBench -p 1 -r 40000000 -c 1 -g 100000 -n binary -s 1024 -x ]

allocator stack :
2018-09-24 14:50:56 51936 INFO  ArrowReader:168 - number of arrow block are : 400 who sets these?
java.lang.Exception
	at com.github.animeshtrivedi.benchmark.DebugAllocatorListener.onAllocation(DebugAllocatorListener.java:36)
	at org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:285)
	at org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:248)
	at org.apache.arrow.vector.ipc.message.MessageSerializer.deserializeRecordBatch(MessageSerializer.java:290)
	at org.apache.arrow.vector.ipc.ArrowFileReader.readRecordBatch(ArrowFileReader.java:162)
	at org.apache.arrow.vector.ipc.ArrowFileReader.loadNextBatch(ArrowFileReader.java:113)
	at org.apache.arrow.vector.ipc.ArrowFileReader.loadRecordBatch(ArrowFileReader.java:139)
	at com.github.animeshtrivedi.benchmark.ArrowReader.run(ArrowReader.java:171)
	at com.github.animeshtrivedi.benchmark.ArrowMemoryBench.run(ArrowMemoryBench.java:76)

What is the holder performance scalability:

 1 core : 13.21
 2 cores: 23.9
 4 cores: 41.3
 8 cores: 59.61
16 cores: 47.21

Major: major performance break-through came when I tried around with the block size. So I varied -g to 10k and -s 1k
 that gives me the footprint of ~1MB, then the performance became

 16 cores -> 130 Gbps !

 previous reported numbers were for -r 40000000 -c 1 -g 100000 (so the 100K buffer size, hence 10MB footprint)

 So there is something about the batch size that determines the performance. We need to explore it more systematically.

 Key factors to evaluate (a) batch size and mem footprint; (2) on vs off heap; (3) GC location; (4) warmup phase, the
 2nd run is always slower than the first.

 Example run:
 numactl -C 0-15 ./run.sh -t ArrowMemBench -p 16 -r 5000000 -c 1 -g 10000 -n binary -s 1024 -e -b
 -----------------------------------------------------------------------
 Total bytes: 81920000000(76GiB) bandwidth 152.99 Gbps
 -----------------------------------------------------------------------

************************************************************************************************************************
September 28, 2018 notes for the mailing list:
[c0923533a29ec0623c3161c5498c8650cf4716a9]
numactl -C 0 ./run.sh -t ArrowMemBench -p 1 -r 10000000 -c 1 -g 10000 -n binary -s 1024 -e -b

26.43 Gbps

Numa memory setup investigation:

java -Xmn32G -Xmx100G -XX:+UseNUMA
java -Xmn32G -Xmx100G -XX:+UseNUMA -cp ./4j/:./benchmark-arrow-1.0.jar:./dependency/* com.github.animeshtrivedi.benchmark.Main -t ArrowMemBench -p 1 -r 40000000 -c 1 -g 100000 -n binary -s 1024 -e

local vs remote NUMA node traffic: https://www.spinics.net/lists/linux-perf-users/msg03351.html
(count node-load/store-misses)





